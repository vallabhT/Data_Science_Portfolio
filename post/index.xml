<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Sri Vallabh Tiwari</title>
    <link>https://vallabht.github.io/Data_Science_Portfolio/post/</link>
    <description>Recent content in Projects on Sri Vallabh Tiwari</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://vallabht.github.io/Data_Science_Portfolio/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 3: An in-depth analysis on the Domestic Student Market</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-3/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-3/</guid>
      <description>Objective: The goal of the project was to utilise the dataset to identify the market share of The University of Melbourne, predictive modelling on the application data to estimate the student enrolment rate and inferring the factors that are responsible for the enrolment rate. The dataset was a large excel file which had over 450,000 rows and 350 columns. It had de-identified data on the offer to enrolment outcomes of students who applied together with their demographic data via the Victorian Tertiary Admission Centre (VTAC) for the undergraduate programs for the last 5 years.</description>
    </item>
    
    <item>
      <title>Project 2: Automatic Fact Verification</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-2/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-2/</guid>
      <description>Build a fact verification system that automatically validates whether a claim is true, false, or unverifiable based on the information in the given text corpus. Built a NLP model, that reads in the whole text corpus and preprocesses it and generates a TF-IDF matrix of the corpus. Cosine Similarity is used on the TF-IDF matrix to identify similar statements and query them. Text classification is performed by using ULMFiT, which is a pre-trained language model.</description>
    </item>
    
    <item>
      <title>Project 1: The Seven Deadly Sins</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-1/</link>
      <pubDate>Wed, 15 May 2019 10:58:08 -0400</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-1/</guid>
      <description>Implementation of a cloud-based system that contains - Twitter harvesting application that can harvest tweets from major cities of Australia, a NoSQL database that stores the tweets and a frontend web application that can be used to visualise the scenarios of choice. Developed a Twitter harvester that harvests tweets from Melbourne and Sydney, that filters tweets based on given keywords, pre-processes them, performs sentiment analysis and stores these tweets in the CouchDB database.</description>
    </item>
    
  </channel>
</rss>
