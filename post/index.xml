<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Sri Vallabh Tiwari</title>
    <link>https://vallabht.github.io/Data_Science_Portfolio/post/</link>
    <description>Recent content in Projects on Sri Vallabh Tiwari</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Mar 2017 12:00:00 -0500</lastBuildDate><atom:link href="https://vallabht.github.io/Data_Science_Portfolio/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Project 6: An in-depth analysis on the Domestic Student Market</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-6/</link>
      <pubDate>Wed, 04 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-6/</guid>
      <description>Objective: The goal of the project was to utilise the dataset to identify the market share of The University of Melbourne, predictive modelling on the application data to estimate the student enrolment rate and inferring the factors that are responsible for the enrolment rate. The dataset was a large excel file which had over 450,000 rows and 350 columns. It had de-identified data on the offer to enrolment outcomes of students who applied together with their demographic data via the Victorian Tertiary Admission Centre (VTAC) for the undergraduate programs for the last 5 years.</description>
    </item>
    
    <item>
      <title>Project 5: Let&#39;s Park Melbourne!</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-5/</link>
      <pubDate>Mon, 25 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-5/</guid>
      <description>The aim of the project was to produce an interactive interfact that could help local citizens that commute to the CBD. Built a Tableau dashboard that could help the commuters identify the available on-street parking spots that are available for use in the city in real time. The interface also consist of two more dashboards that provide historical data about each on-street parking spots and also helps the user to predict the availablity of the spots.</description>
    </item>
    
    <item>
      <title>Project 4: Traffic Collisions in Victoria</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-4/</link>
      <pubDate>Thu, 09 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-4/</guid>
      <description>The aim of the analysis was to build a dashboard that can help VicRoads to identify different patterns that have been persistent in the traffic accidents that have taken place in the last 6 years. The dashboard that I built helps us identify the social crash cost of accidents at different locations, varieties of the road geomerty and the attmospheric conditions that affect the causation of an accident. There were over 9 datasets used in analysis.</description>
    </item>
    
    <item>
      <title>Project 3: Who Tweeted That?</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-3/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-3/</guid>
      <description>The goal of the project was to develope a machine learning model that can predict the author of a tweet from the given training tweets. I built a multiclass classification model based on different text classifiers, that uses the newly engineered features based on the textual data. The features were lexical features, syntatical features, and content specific features. Doc2Vec was a part of feature engineering. Model selection was done based on the time it took to train the model and the one with the best trade-off between bias and variance.</description>
    </item>
    
    <item>
      <title>Project 2: Automatic Fact Verification</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-2/</link>
      <pubDate>Wed, 22 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-2/</guid>
      <description>Build a fact verification system that automatically validates whether a claim is true, false, or unverifiable based on the information in the given text corpus. Built a NLP model, that reads in the whole text corpus and preprocesses it and generates a TF-IDF matrix of the corpus. Cosine Similarity is used on the TF-IDF matrix to identify similar statements and query them. Text classification is performed by using ULMFiT, which is a pre-trained language model.</description>
    </item>
    
    <item>
      <title>Project 1: The Seven Deadly Sins</title>
      <link>https://vallabht.github.io/Data_Science_Portfolio/post/project-1/</link>
      <pubDate>Wed, 15 May 2019 10:58:08 -0400</pubDate>
      
      <guid>https://vallabht.github.io/Data_Science_Portfolio/post/project-1/</guid>
      <description>Implementation of a cloud-based system that contains - Twitter harvesting application that can harvest tweets from major cities of Australia, a NoSQL database that stores the tweets and a frontend web application that can be used to visualise the scenarios of choice. Developed a Twitter harvester that harvests tweets from Melbourne and Sydney, that filters tweets based on given keywords, pre-processes them, performs sentiment analysis and stores these tweets in the CouchDB database.</description>
    </item>
    
  </channel>
</rss>
